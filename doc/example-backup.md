# ðŸ’¾ Example of a backup

**[Back](../README.md)**

  * In this example you are in the directoy: /Users/rtitz/
  * The following files are in this directory:
```text
â”œâ”€â”€ aws-s3-backup_macos-arm64 <- This is the aws-s3-backup executable
â”œâ”€â”€ dir <- This is a directory containing files and sub directories
â”‚Â Â  â”œâ”€â”€ dir1 <- This is an empty directory
â”‚Â Â  â”œâ”€â”€ dir2 <- This is a directory containing files
â”‚Â Â  â”‚Â Â  â””â”€â”€ new-file
â”‚Â Â  â”œâ”€â”€ file1
â”‚Â Â  â”œâ”€â”€ file2
â”‚Â Â  â”œâ”€â”€ file3
â”œâ”€â”€ fileUtils <- This is a directory containing files 
â”‚Â Â  â”œâ”€â”€ fileInfo.go
â”‚Â Â  â”œâ”€â”€ fileSizeUnit.go
â”‚Â Â  â”œâ”€â”€ splitArchives.go
â”‚Â Â  â””â”€â”€ tar.go
â”œâ”€â”€ my-input.json <- This is the input.json for aws-s3-backup
â”œâ”€â”€ test-directory <- This is a directory containing files and sub directories
â”‚Â Â  â”œâ”€â”€ NEW
â”‚Â Â  â”œâ”€â”€ go.mod
â”‚Â Â  â”œâ”€â”€ go.sum
â”‚Â Â  â”œâ”€â”€ main.go
â”‚Â Â  â”œâ”€â”€ notifications
â”‚Â Â  â”‚Â Â  â””â”€â”€ mail.go
â”‚Â Â  â””â”€â”€ test.file
â”œâ”€â”€ test.file <- This is a file
â”œâ”€â”€â”€ test2.file <- This is another file
â””â”€â”€ tmp <- This is an empty directory
```

  * ðŸ“„ Here is how your my-input.json could look like:
```json
{
    "tasks": [
        {
            "S3Bucket": "my-s3-backup-bucket",
            "S3Prefix": "backup",
            "StorageClass": "DEEP_ARCHIVE",
            "ArchiveSplitEachMB": "250",
            "TmpStorageToBuildArchives": "/Users/rtitz/tmp",
            "CleanupTmpStorage": "true",
            "Content": [
                "/Users/rtitz/test-directory",
                "/Users/rtitz/test.file",
                "/Users/rtitz/test2.file",
                "/Users/rtitz/dir/dir2/new-file"
            ]
        }
    ]
}
```

  * â–¶ï¸ Here is how to execute this:
```bash
export AWS_ACCESS_KEY_ID="AKXXXXXXXXX"
export AWS_SECRET_ACCESS_KEY="XXXXXXXXXXXXXXXXX"

# Normal backup
aws-s3-backup_macos-arm64 -json my-input.json

# Or test with dry-run (no S3 upload)
aws-s3-backup_macos-arm64 -json my-input.json -dryrun 
```

  * ðŸ“ Output will be similar to:
```
AWS-S3-BACKUP 1.3.4

MODE: BACKUP
REGION: us-east-1

ðŸ“¦ Creating archive: test-directory.tar.gz
âž¡ï¸ Adding to archive: test-directory/NEW
âž¡ï¸ Adding to archive: test-directory/go.mod
âœ… Archive created successfully: test-directory.tar.gz
â¬†ï¸ Uploading (1/4): test-directory.tar.gz (2.30 KB)
âœ… Upload successful: test-directory.tar.gz

==================================================
ðŸ“Š BACKUP SUMMARY
==================================================
âœ… Successfully uploaded: 4
ðŸ“ Total files processed: 5
ðŸ’¾ Total data processed: 3.67 KB
â±ï¸ Preparation time: 1.2s
â±ï¸ Upload time: 2.3s
â±ï¸ Total time: 3.5s

ðŸŽ‰ Backup completed successfully!
==================================================
```

  * ðŸ“‹ Dry-run output would show:
```
AWS-S3-BACKUP 1.3.4

MODE: BACKUP
REGION: DRY-RUN

ðŸ“¦ Creating archive: test-directory.tar.gz
âž¡ï¸ Adding to archive: test-directory/NEW
âœ… Archive created successfully: test-directory.tar.gz
â¬†ï¸ [DRY-RUN] Would upload (1/4): test-directory.tar.gz (2.30 KB)
â¬†ï¸ [DRY-RUN] Would upload additional file: my-input.json

==================================================
ðŸ“‹ BACKUP SUMMARY (DRY-RUN)
==================================================
âœ… Files that would be uploaded: 5
ðŸ“ Total files processed: 5
ðŸ’¾ Total data processed: 3.67 KB
â±ï¸ Preparation time: 1.2s
â±ï¸ Upload time (simulated): 45ms
â±ï¸ Total time: 1.3s

ðŸŽ‰ Dry-run completed successfully!
==================================================

```

  * ðŸ“‹ Dry-run output would show:
```
AWS-S3-BACKUP 1.3.1

MODE: BACKUP

[DRY-RUN] Would upload (1/4): test-directory.tar.gz (2.30 MB) to s3://my-s3-backup-bucket/backup/Users/rtitz/test-directory.tar.gz
[DRY-RUN] Would upload (2/4): test.file.tar.gz (0.57 MB) to s3://my-s3-backup-bucket/backup/Users/rtitz/test.file.tar.gz
[DRY-RUN] Would upload (3/4): test2.file.tar.gz (0.49 MB) to s3://my-s3-backup-bucket/backup/Users/rtitz/test2.file.tar.gz
[DRY-RUN] Would upload (4/4): new-file.tar.gz (0.11 MB) to s3://my-s3-backup-bucket/backup/Users/rtitz/dir/dir2/new-file.tar.gz
[DRY-RUN] Would upload additional file: my-input.json to s3://my-s3-backup-bucket/backup/my-input.json
```

  * âœ… Everything in 'Content' part of the my-input.json is uploaded to S3 bucket "my-s3-backup-bucket" and placed with their full paths into folder "backup" in this bucket.
  * ðŸ”„ **Smart uploads**: If you run the same backup again, existing files will be skipped automatically
  * â­ï¸ **Skip existing files**: Shows `â­ï¸ Skipping: filename (already exists in S3)` for files that don't need re-upload
  * ðŸŒ **Network resilience**: Automatically retries network failures for up to 12 hours
  * ðŸ›¡ï¸ **Safe operations**: Never overwrites existing data, aborts upload if safety cannot be verified
  * ðŸ•°ï¸ **Timestamp preservation**: Original file and directory timestamps stored in archives
  * ðŸ“Š **Enhanced summary**: Displays count of uploaded, skipped, and failed files

**[Back](../README.md)**
## ðŸ”„ Smart Upload Example

When running the same backup multiple times:

**First run:**
```
â¬†ï¸ Uploading (1/3): test-directory.tar.gz (2.30 KB)
âœ… Upload successful: test-directory.tar.gz
â¬†ï¸ Uploading (2/3): test.file.tar.gz (0.57 KB)
âœ… Upload successful: test.file.tar.gz
â¬†ï¸ Uploading additional file: my-input.json
âœ… Additional file uploaded: my-input.json

ðŸ“Š BACKUP SUMMARY
âœ… Successfully uploaded: 3
ðŸ“ Total files processed: 3
```

**Second run (same files):**
```
â­ï¸ Skipping (1/3): test-directory.tar.gz (already exists in S3)
â­ï¸ Skipping (2/3): test.file.tar.gz (already exists in S3)
â­ï¸ Skipping additional file: my-input.json (already exists in S3)

ðŸ“Š BACKUP SUMMARY
â­ï¸ Skipped (already exists): 3
ðŸ“ Total files processed: 3
```

This prevents unnecessary uploads and reduces costs significantly.
## ðŸŒ Network Interruption Handling

If network connection is lost during backup:

```
â¬†ï¸ Uploading (3/10): file003.tar.gz (1.8 MB)
âš ï¸ Upload file003.tar.gz failed (attempt 1): dial tcp: network is unreachable
ðŸ”„ Retrying in 1s... (Press Ctrl+C to cancel, will retry for up to 12 hours)
âš ï¸ Upload file003.tar.gz failed (attempt 2): dial tcp: network is unreachable  
ðŸ”„ Retrying in 2s... (Press Ctrl+C to cancel, will retry for up to 12 hours)
[Network restored]
âœ… Upload file003.tar.gz succeeded after 3 attempts
â¬†ï¸ Uploading (4/10): file004.tar.gz (2.1 MB)
```

**Benefits:**
- No manual intervention needed for temporary network issues
- Completed uploads are never lost or repeated
- User can cancel anytime with Ctrl+C
- Automatic recovery when network returns
## ðŸ” Encrypted Backup Example

Using encryption in your input.json:

```json
{
    "tasks": [
        {
            "S3Bucket": "my-secure-backup-bucket",
            "S3Prefix": "encrypted-backup",
            "StorageClass": "STANDARD_IA",
            "ArchiveSplitEachMB": "100",
            "TmpStorageToBuildArchives": "/tmp/backup",
            "CleanupTmpStorage": "true",
            "EncryptionSecret": "MyS3cureP@ssw0rd2024!",
            "Content": [
                "/home/user/important-documents",
                "/home/user/photos"
            ]
        }
    ]
}
```

**Output with encryption:**
```
ðŸ“¦ Creating archive: important-documents.tar.gz
âœ… Archive created successfully: important-documents.tar.gz
ðŸ”’ Encrypting file: important-documents.tar.gz
âœ… File encrypted successfully: important-documents.tar.gz.enc
â¬†ï¸ Uploading (1/2): important-documents.tar.gz.enc (15.2 MB)
âœ… Upload successful: important-documents.tar.gz.enc
```

**âš ï¸ CRITICAL WARNINGS:**
- **Password required for restore** - Keep it safe and accessible
- **Manual decryption scripts provided** - `decrypt_manual.py` and `decrypt_openssl.sh`
- **Test your setup** - Verify you can restore before relying on encryption
- **No password recovery** - If you lose the password, your data is permanently inaccessible